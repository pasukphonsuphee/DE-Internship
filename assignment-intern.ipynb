{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "import scala.collection.JavaConversions._\n",
    "import java.io.{BufferedInputStream, OutputStreamWriter}\n",
    "import java.io.BufferedWriter\n",
    "import scala.collection.mutable.ListBuffer\n",
    "import java.time.LocalDate\n",
    "import java.time.temporal.ChronoUnit.DAYS\n",
    "import scala.util.Random\n",
    "import java.time.format.DateTimeFormatter\n",
    "import au.com.bytecode.opencsv.CSVWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hadoop = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-830363801_1, ugi=ext_pasuk_pho (auth:SIMPLE)]]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "randomMsisdn: (len: Int)String\n",
       "randomCard: (len: Int)String\n",
       "randomDate: (from: java.time.LocalDate, to: java.time.LocalDate)java.time.LocalDate\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-830363801_1, ugi=ext_pasuk_pho (auth:SIMPLE)]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/////////////////////////////////////////////////////////finish/////////////////////////////////////////////////////////////////\n",
    "val hadoop : FileSystem = {\n",
    "    val conf = new Configuration( )\n",
    "    conf.set( \"fs.defaultFS\", \"hdfs://tapprod\" )\n",
    "    FileSystem.get( conf )\n",
    "  }\n",
    "\n",
    "def randomMsisdn(len: Int): String = {\n",
    "   val random = new Random(System.nanoTime)\n",
    "   val sb = new StringBuilder(len)\n",
    "   val ab = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "   for (i <- 0 until len) {\n",
    "     sb.append(ab(random.nextInt(ab.length)))\n",
    "   }\n",
    "   sb.toString\n",
    "}\n",
    "\n",
    "def randomCard(len: Int): String = {\n",
    "   val random = new Random(System.nanoTime)\n",
    "   val sb = new StringBuilder(len)\n",
    "   val ab = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "   for (i <- 0 until len) {\n",
    "     sb.append(ab(random.nextInt(ab.length)))\n",
    "   }\n",
    "   sb.toString\n",
    "}\n",
    "\n",
    "def randomDate(from: LocalDate, to: LocalDate): LocalDate = {\n",
    "    val diff = DAYS.between(from, to)\n",
    "    val random = new Random(System.nanoTime)\n",
    "    from.plusDays(random.nextInt(diff.toInt))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "write: (filename: String)Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// optimization\n",
    "def write( filename : String) = {\n",
    "    val path = new Path(filename)\n",
    "    val outputFile = new BufferedWriter(new OutputStreamWriter( hadoop.create( path, false ) )) \n",
    "    val csvWriter = new CSVWriter(outputFile)\n",
    "    val csvFields = Array(\"msisdn\", \"card_id\", \"province\", \"name\",\"birth day\") \n",
    "    val ab = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n",
    "    \n",
    "    val provinceList = List(\"Bangkok\", \"Samut Prakan\", \"Pathum Thani\", \"Nakhon Pathom\",\"Rayong\",\n",
    "                            \"Prachinburi\",\"Khon Kaen\",\"Loei\",\"Kalasin\", \"Chiang Rai\",\"Nan\",\"Phuket\",\n",
    "                            \"Ranong\",\"Krabi\",\"Yala\",\"Pattani\",\"Phetchaburi\")\n",
    "    \n",
    "    val nameList = List(\"Roger\",\"Terry\",\"Keith\",\"Nathan\",\"Jerry\",\"Alexander\",\"Benjamin\",\"Jonathan\",\n",
    "                        \"Edward\",\"Kenneth\",\"Anthony\",\"Michael\",\"William\",\"Willie\",\"Billy\")\n",
    "    val random = new Random()\n",
    "    val DatePattern: String = \"dd-MM-yyyy\"\n",
    "    val DateFormatter = DateTimeFormatter.ofPattern(DatePattern)\n",
    "    val start = LocalDate.of(1960,1,1)\n",
    "    val end = LocalDate.of(2010,12,31)\n",
    "    var listOfRecords = new ListBuffer[Array[String]]()\n",
    "    listOfRecords += csvFields\n",
    "    // (1 to 10).foreach()\n",
    "    (1 to 10).foreach(_=> listOfRecords += Array(\"0\" + randomMsisdn(9)\n",
    "                           , randomCard(10)\n",
    "                           , provinceList(random.nextInt(provinceList.length))\n",
    "                           , nameList(random.nextInt(nameList.length))\n",
    "                           , DateFormatter.format(randomDate(start,end)).toString))\n",
    "    csvWriter.writeAll(listOfRecords.toList)\n",
    "    outputFile.close()\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "write(\"/user/ext_pasuk_pho/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "import scala.collection.JavaConversions._\n",
    "import java.io.{BufferedInputStream, OutputStreamWriter}\n",
    "import java.io.BufferedWriter\n",
    "import scala.collection.mutable.ListBuffer\n",
    "import java.time.LocalDate\n",
    "import java.time.temporal.ChronoUnit.DAYS\n",
    "import scala.util.Random\n",
    "import java.time.format.DateTimeFormatter\n",
    "import au.com.bytecode.opencsv.CSVWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hadoop = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_453774249_1, ugi=ext_pasuk_pho (auth:SIMPLE)]]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "randomMsisdn: (len: Int)String\n",
       "randomCard: (len: Int)String\n",
       "randomDate: (from: java.time.LocalDate, to: java.time.LocalDate)java.time.LocalDate\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_453774249_1, ugi=ext_pasuk_pho (auth:SIMPLE)]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hadoop : FileSystem = {\n",
    "    val conf = new Configuration( )\n",
    "    conf.set( \"fs.defaultFS\", \"hdfs://tapprod\" )\n",
    "    FileSystem.get( conf )\n",
    "  }\n",
    "\n",
    "def randomMsisdn(len: Int): String = {\n",
    "   val random = new Random(System.nanoTime)\n",
    "   val sb = new StringBuilder(len)\n",
    "   val ab = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "   for (i <- 0 until len) {\n",
    "     sb.append(ab(random.nextInt(ab.length)))\n",
    "   }\n",
    "   sb.toString\n",
    "}\n",
    "\n",
    "def randomCard(len: Int): String = {\n",
    "   val random = new Random(System.nanoTime)\n",
    "   val sb = new StringBuilder(len)\n",
    "   val ab = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "   for (i <- 0 until len) {\n",
    "     sb.append(ab(random.nextInt(ab.length)))\n",
    "   }\n",
    "   sb.toString\n",
    "}\n",
    "\n",
    "def randomDate(from: LocalDate, to: LocalDate): LocalDate = {\n",
    "    val diff = DAYS.between(from, to)\n",
    "    val random = new Random(System.nanoTime)\n",
    "    from.plusDays(random.nextInt(diff.toInt))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Employee\n",
       "spark = org.apache.spark.sql.SparkSession@68881c08\n",
       "provinceList = List(Bangkok, Samut Prakan, Pathum Thani, Nakhon Pathom, Rayong, Prachinburi, Khon Kaen, Loei, Kalasin, Chiang Rai, Nan, Phuket, Ranong, Krabi, Yala, Pattani, Phetchaburi)\n",
       "random = scala.util.Random@6edb61b7\n",
       "employeesData = Vector(Employee(dqwmszmao,4PROBGWN0Q,Loei), Employee(eloierkiv,MUDN4NUL33,Ranong), Employee(zlceeqxrv,4MVV2JFUGF,Prachinburi), Employee(ugzqktxpi,PZ3QFIML08,Phuket), Employee(alpzcalpj,YZ5PUXKLT6,Ranong), Employee(qfdqohman,V78PBB5B3Y,Chiang Rai), Employee(oywehfmoa,6OCFDGJ1ON,Loei), Empl...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Vector(Employee(dqwmszmao,4PROBGWN0Q,Loei), Employee(eloierkiv,MUDN4NUL33,Ranong), Employee(zlceeqxrv,4MVV2JFUGF,Prachinburi), Employee(ugzqktxpi,PZ3QFIML08,Phuket), Employee(alpzcalpj,YZ5PUXKLT6,Ranong), Employee(qfdqohman,V78PBB5B3Y,Chiang Rai), Employee(oywehfmoa,6OCFDGJ1ON,Loei), Empl..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{SaveMode, SparkSession}\n",
    "import scala.util.Random\n",
    "case class Employee(msisdn: String,card_id: String,province: String)//,name: String, birth day:String\n",
    "val spark = SparkSession.builder().master(\"local\").getOrCreate()\n",
    "\n",
    "val provinceList = List(\"Bangkok\", \"Samut Prakan\", \"Pathum Thani\", \"Nakhon Pathom\",\"Rayong\",\n",
    "                            \"Prachinburi\",\"Khon Kaen\",\"Loei\",\"Kalasin\", \"Chiang Rai\",\"Nan\",\"Phuket\",\n",
    "                            \"Ranong\",\"Krabi\",\"Yala\",\"Pattani\",\"Phetchaburi\")\n",
    "val random = new Random()\n",
    "\n",
    "val employeesData = (1 to 100).map(c => Employee(randomMsisdn(9)\n",
    "                                   ,randomCard(10)\n",
    "                                   ,provinceList(random.nextInt(provinceList.length))))\n",
    "import spark.implicits._\n",
    "val df = employeesData.toDF().repartition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------------+\n",
      "|   msisdn|   card_id|     province|\n",
      "+---------+----------+-------------+\n",
      "|etnjvgxac|9530M1W9WH|         Loei|\n",
      "|lckudrzpr|AHBP8DW44Q| Samut Prakan|\n",
      "|keqhmarus|ST0LV7SSX4|          Nan|\n",
      "|wltykfudm|19M3D7XSVS|Nakhon Pathom|\n",
      "|hmzcggvyp|N1PO47SRFA|       Ranong|\n",
      "|vzcqitqrb|ETC5CPM41F|      Kalasin|\n",
      "|irmkclkec|VXICJGEMZ0|      Pattani|\n",
      "|wpgutunvv|GMR6J8MFO3|       Phuket|\n",
      "|ushsvzlqy|612PH1WVJ5|        Krabi|\n",
      "|lwtepqbzt|NNIB3MU845|  Phetchaburi|\n",
      "|yznliukxo|O2U45X3Q3Y|          Nan|\n",
      "|miwdeqfxd|QR301JXZEU|       Ranong|\n",
      "|vivmngaer|2ZMMEWO9AF|         Loei|\n",
      "|qkhmeklkf|E6E3TI5VRX|      Kalasin|\n",
      "|bjptwfxrh|N5VZE63MRQ| Pathum Thani|\n",
      "|liogagfxa|DD3M835UTH|         Yala|\n",
      "|vmvhtupim|S7MM6628XJ|        Krabi|\n",
      "|ccfaekvwe|AGX7QAEGL1|  Prachinburi|\n",
      "|yeisrymak|K960YTNFFP| Samut Prakan|\n",
      "|svfhnhcpn|AODQPROY17|       Ranong|\n",
      "+---------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "savePath = hdfs://tapprod/user/ext_pasuk_pho/test.csv\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "hdfs://tapprod/user/ext_pasuk_pho/test.csv"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val savePath = \"hdfs://tapprod/user/ext_pasuk_pho/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.write\n",
    "        .format(\"csv\")\n",
    "        .mode(SaveMode.Overwrite)\n",
    "        .option(\"compression\", \"uncompressed\")\n",
    "        .save(savePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala (YARN Cluster Mode)",
   "language": "scala",
   "name": "spark_scala_yarn_cluster"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
